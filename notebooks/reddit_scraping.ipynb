{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import praw\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "CLIENT_ID=os.environ.get('REDDIT_CLIENT_ID')\n",
    "SECRET_KEY=os.environ.get('REDDIT_SECRET_KEY')\n",
    "pw = os.environ.get('REDDIT_PW')\n",
    "username = os.environ.get('REDDIT_USERNAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Scraping last (x) posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If SSL Connection works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up access to the Reddit API\n",
    "auth=requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_KEY)\n",
    "\n",
    "data={\n",
    "    'grant_type': 'password',\n",
    "    'username': username,\n",
    "    'password': pw\n",
    "}\n",
    "\n",
    "headers = {'User-Agent':'MyAPI/0.0.1'}\n",
    "\n",
    "\n",
    "res=requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                  auth=auth, data=data, headers=headers)\n",
    "\n",
    "TOKEN=res.json()['access_token']\n",
    "\n",
    "headers['Authorization']=f'bearer {TOKEN}'\n",
    "\n",
    "reddit=praw.Reddit(user_agent=\"Get Comments\",client_id=CLIENT_ID,client_secret=SECRET_KEY)\n",
    "\n",
    "# hotpost=requests.get('https://oauth.reddit.com/r/CryptoCurrency/search/?q=Bitcoin&sort=new',\n",
    "#                  headers=headers, params={'restrict_sr':'1','limit':'100'}).json()\n",
    "\n",
    "hotpost=requests.get('https://oauth.reddit.com/r/CryptoCurrency?sort=new',\n",
    "                 headers=headers, params={'restrict_sr':'1','limit':'10'}).json()\n",
    "# for post in hotpost['data']['children']:\n",
    "#     print(post['data']['title'])\n",
    "\n",
    "df_posts_2 = pd.DataFrame()\n",
    "\n",
    "for post in hotpost['data']['children']:\n",
    "    submission = reddit.submission(id=post['data']['id'])\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    df_posts_2=df_posts_2._append({\n",
    "        'subreddit':post['data']['subreddit'],\n",
    "        'title':post['data']['title'],\n",
    "        'timestamp':post['data']['created_utc'],\n",
    "        'selftext':post['data']['selftext'],\n",
    "        'upvote_ratio':post['data']['upvote_ratio'],\n",
    "        'ups':post['data']['ups'],\n",
    "        'downs':post['data']['downs'],\n",
    "        'score':post['data']['score'],\n",
    "        'id':post['data']['id'],\n",
    "        'name':post['data']['name']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "\n",
    "df_comments = pd.DataFrame()\n",
    "\n",
    "for post in hotpost['data']['children']:\n",
    "    submission = reddit.submission(id=post['data']['id'])\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments.list():\n",
    "        df_comments=df_comments._append({\n",
    "            'subreddit':post['data']['subreddit'],\n",
    "            'title':post['data']['title'],\n",
    "            'timestamp':post['data']['created_utc'],\n",
    "            'selftext':post['data']['selftext'],\n",
    "            'upvote_ratio':post['data']['upvote_ratio'],\n",
    "            'ups':post['data']['ups'],\n",
    "            'downs':post['data']['downs'],\n",
    "            'score':post['data']['score'],\n",
    "            'id':post['data']['id'],\n",
    "            'name':post['data']['name'],\n",
    "            'comment':comment.body,\n",
    "            'comment_ups':comment.ups,\n",
    "            'comment_downs':comment.downs,\n",
    "            'comment_timestamp':comment.created_utc\n",
    "            }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If SSL Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "url = 'https://www.reddit.com/r/CryptoCurrency/new/'\n",
    "\n",
    "# Create a session with retry mechanism\n",
    "session = requests.Session()\n",
    "retry = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[502, 503, 504]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "#Setting up access to the Reddit API\n",
    "auth=requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_KEY)\n",
    "\n",
    "data={\n",
    "    'grant_type': 'password',\n",
    "    'username': username,\n",
    "    'password': pw\n",
    "}\n",
    "\n",
    "headers = {'User-Agent':'MyAPI/0.0.1'}\n",
    "\n",
    "res=session.post('https://www.reddit.com/api/v1/access_token',\n",
    "                  auth=auth, data=data, headers=headers)\n",
    "\n",
    "TOKEN=res.json()['access_token']\n",
    "\n",
    "headers['Authorization']=f'bearer {TOKEN}'\n",
    "\n",
    "reddit=praw.Reddit(user_agent=\"Get Comments\",client_id=CLIENT_ID,client_secret=SECRET_KEY)\n",
    "\n",
    "hotpost=session.get('https://oauth.reddit.com/r/CryptoCurrency?sort=new',\n",
    "                 headers=headers, params={'restrict_sr':'1','limit':'10'}).json()\n",
    "\n",
    "df_posts_2 = pd.DataFrame()\n",
    "\n",
    "for post in hotpost['data']['children']:\n",
    "    submission = reddit.submission(id=post['data']['id'])\n",
    "    try:\n",
    "        submission.comments.replace_more(limit=None)\n",
    "    except:\n",
    "        pass\n",
    "    df_posts_2=df_posts_2._append({\n",
    "        'subreddit':post['data']['subreddit'],\n",
    "        'title':post['data']['title'],\n",
    "        'timestamp':post['data']['created_utc'],\n",
    "        'selftext':post['data']['selftext'],\n",
    "        'upvote_ratio':post['data']['upvote_ratio'],\n",
    "        'ups':post['data']['ups'],\n",
    "        'downs':post['data']['downs'],\n",
    "        'score':post['data']['score'],\n",
    "        'id':post['data']['id'],\n",
    "        'name':post['data']['name']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "\n",
    "df_comments = pd.DataFrame()\n",
    "\n",
    "for post in hotpost['data']['children']:\n",
    "    submission = reddit.submission(id=post['data']['id'])\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments.list():\n",
    "        df_comments=df_comments._append({\n",
    "            'subreddit':post['data']['subreddit'],\n",
    "            'title':post['data']['title'],\n",
    "            'timestamp':post['data']['created_utc'],\n",
    "            'selftext':post['data']['selftext'],\n",
    "            'upvote_ratio':post['data']['upvote_ratio'],\n",
    "            'ups':post['data']['ups'],\n",
    "            'downs':post['data']['downs'],\n",
    "            'score':post['data']['score'],\n",
    "            'id':post['data']['id'],\n",
    "            'name':post['data']['name'],\n",
    "            'comment':comment.body,\n",
    "            'comment_ups':comment.ups,\n",
    "            'comment_downs':comment.downs,\n",
    "            'comment_timestamp':comment.created_utc\n",
    "            }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "\n",
    "df_posts_2['timestamp'] = pd.to_datetime(df_posts_2['timestamp'], unit='s')\n",
    "df_comments['timestamp'] = pd.to_datetime(df_comments['timestamp'], unit='s')\n",
    "df_comments['comment_timestamp'] = pd.to_datetime(df_comments['comment_timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 18:59:22,054 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-08-15 18:59:22,055 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"reddit_comments\")\n",
      "2024-08-15 18:59:22,056 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2024-08-15 18:59:22,060 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"reddit_comments\")\n",
      "2024-08-15 18:59:22,061 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2024-08-15 18:59:22,063 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "2024-08-15 18:59:22,066 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-08-15 18:59:22,069 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE reddit_comments (\n",
      "\t\"index\" BIGINT, \n",
      "\tsubreddit TEXT, \n",
      "\ttitle TEXT, \n",
      "\ttimestamp DATETIME, \n",
      "\tselftext TEXT, \n",
      "\tupvote_ratio FLOAT, \n",
      "\tups BIGINT, \n",
      "\tdowns BIGINT, \n",
      "\tscore BIGINT, \n",
      "\tid TEXT, \n",
      "\tname TEXT, \n",
      "\tcomment TEXT, \n",
      "\tcomment_ups BIGINT, \n",
      "\tcomment_downs BIGINT, \n",
      "\tcomment_timestamp DATETIME\n",
      ")\n",
      "\n",
      "\n",
      "2024-08-15 18:59:22,072 INFO sqlalchemy.engine.Engine [no key 0.00318s] ()\n",
      "2024-08-15 18:59:22,085 INFO sqlalchemy.engine.Engine CREATE INDEX ix_reddit_comments_index ON reddit_comments (\"index\")\n",
      "2024-08-15 18:59:22,087 INFO sqlalchemy.engine.Engine [no key 0.00144s] ()\n",
      "2024-08-15 18:59:22,100 INFO sqlalchemy.engine.Engine COMMIT\n",
      "2024-08-15 18:59:22,110 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-08-15 18:59:22,120 INFO sqlalchemy.engine.Engine INSERT INTO reddit_comments (\"index\", subreddit, title, timestamp, selftext, upvote_ratio, ups, downs, score, id, name, comment, comment_ups, comment_downs, comment_timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
      "2024-08-15 18:59:22,121 INFO sqlalchemy.engine.Engine [generated in 0.00666s] [(0, 'CryptoCurrency', 'Why Bitcoin May Fall To $54,000?', '2024-08-15 14:37:17.000000', '', 0.26, 0, 0, 0, '1eswu5v', 't3_1eswu5v', 'We already dropped to 49k just 10 days ago lmao', 14, 0, '2024-08-15 14:44:05.000000'), (1, 'CryptoCurrency', 'Why Bitcoin May Fall To $54,000?', '2024-08-15 14:37:17.000000', '', 0.26, 0, 0, 0, '1eswu5v', 't3_1eswu5v', 'Counterpoint…  whatever that article says could also send it back to $64k.', 10, 0, '2024-08-15 14:48:21.000000'), (2, 'CryptoCurrency', 'Why Bitcoin May Fall To $54,000?', '2024-08-15 14:37:17.000000', '', 0.26, 0, 0, 0, '1eswu5v', 't3_1eswu5v', 'Can we ban these articles? \\xa0', 4, 0, '2024-08-15 15:56:46.000000'), (3, 'CryptoCurrency', 'Why Bitcoin May Fall To $54,000?', '2024-08-15 14:37:17.000000', '', 0.26, 0, 0, 0, '1eswu5v', 't3_1eswu5v', \"$Bitcoin may also go to $70k. I just hope it doesn't go crazy before I get my bonus at the end of September...\", 3, 0, '2024-08-15 15:00:29.000000'), (4, 'CryptoCurrency', 'Why Bitcoin May Fall To $54,000?', '2024-08-15 14:37:17.000000', '', 0.26, 0, 0, 0, '1eswu5v', 't3_1eswu5v', \"Call me when it's 10k or 100k.\", 3, 0, '2024-08-15 15:27:45.000000'), (5, 'CryptoCurrency', 'Why Bitcoin May Fall To $54,000?', '2024-08-15 14:37:17.000000', '', 0.26, 0, 0, 0, '1eswu5v', 't3_1eswu5v', 'What is this thumbnail even did they have a stroke', 2, 0, '2024-08-15 15:12:20.000000'), (6, 'CryptoCurrency', 'Why Bitcoin May Fall To $54,000?', '2024-08-15 14:37:17.000000', '', 0.26, 0, 0, 0, '1eswu5v', 't3_1eswu5v', 'Well because it might shoot to 500k as well?', 2, 0, '2024-08-15 15:18:20.000000'), (7, 'CryptoCurrency', 'Why Bitcoin May Fall To $54,000?', '2024-08-15 14:37:17.000000', '', 0.26, 0, 0, 0, '1eswu5v', 't3_1eswu5v', 'Probably for the opposite reasons as to why it “may” rise to 100k', 2, 0, '2024-08-15 16:12:17.000000')  ... displaying 10 of 304 total bound parameter sets ...  (302, 'CryptoCurrency', 'Coinbase teases upcoming launch of apparent tokenized bitcoin named cbBTC', '2024-08-14 07:23:53.000000', '', 0.88, 129, 0, 129, '1erved3', 't3_1erved3', 'People deposit WBTC into lending protocols like aave or beefy to earn yield. \\n\\nI’m not saying it’s the best idea, but it’s a thing that exists.', 1, 0, '2024-08-14 19:58:20.000000'), (303, 'CryptoCurrency', 'Coinbase teases upcoming launch of apparent tokenized bitcoin named cbBTC', '2024-08-14 07:23:53.000000', '', 0.88, 129, 0, 129, '1erved3', 't3_1erved3', \"it doesn't have to be custodial. HTLCs enable this. the most popular non-custodial wrapped bitcoin is TBTC. you just have to decide if you are more afraid of counterparty risk or smart contract risk.\", 2, 0, '2024-08-14 17:25:18.000000')]\n",
      "2024-08-15 18:59:22,124 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine=create_engine('sqlite:///../data/reddit_comments.db',echo=True)\n",
    "# sqlite_connection=engine.connect()\n",
    "sqlite_table='reddit_comments'\n",
    "df.to_sql(sqlite_table,engine, if_exists='fail')\n",
    "# sqlite_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 18:50:41,394 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-08-15 18:50:41,396 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"select * from reddit_comments\")\n",
      "2024-08-15 18:50:41,397 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2024-08-15 18:50:41,402 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"select * from reddit_comments\")\n",
      "2024-08-15 18:50:41,404 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2024-08-15 18:50:41,405 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OptionEngine' object has no attribute 'execute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m sqlite_connection\u001b[38;5;241m=\u001b[39mengine\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m      3\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect * from reddit_comments\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/io/sql.py:593\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    585\u001b[0m         sql,\n\u001b[1;32m    586\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m         chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    591\u001b[0m     )\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/io/sql.py:1560\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m args \u001b[38;5;241m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 1560\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/pandas/io/sql.py:1405\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;124;03m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnectable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OptionEngine' object has no attribute 'execute'"
     ]
    }
   ],
   "source": [
    "engine=create_engine('sqlite:///../data/reddit_comments.db',echo=True)\n",
    "sqlite_connection=engine.connect()\n",
    "sql = \"select * from reddit_comments\"\n",
    "df = pd.read_sql(sql,engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 18:46:49,136 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "sqlite_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Pushshifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.reddit.com/r/pushshift/comments/194k9y4/reddit_dump_files_through_the_end_of_2023/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import praw\n",
    "import time\n",
    "\n",
    "# Authentication: http://praw.readthedocs.io/en/latest/getting_started/authentication.html\n",
    "reddit = praw.Reddit(client_id=CLIENT_ID, client_secret=SECRET_KEY,\n",
    "                     password=pw, user_agent='MyAPI/0.0.1',\n",
    "                     username=username)\n",
    "\n",
    "def submissions_pushshift_praw(subreddit, start=None, end=None, limit=100, extra_query=\"\"):\n",
    "    \"\"\"\n",
    "    A simple function that returns a list of PRAW submission objects during a particular period from a defined sub.\n",
    "    This function serves as a replacement for the now deprecated PRAW `submissions()` method.\n",
    "\n",
    "    :param subreddit: A subreddit name to fetch submissions from.\n",
    "    :param start: A Unix time integer. Posts fetched will be AFTER this time. (default: None)\n",
    "    :param end: A Unix time integer. Posts fetched will be BEFORE this time. (default: None)\n",
    "    :param limit: There needs to be a defined limit of results (default: 100), or Pushshift will return only 25.\n",
    "    :param extra_query: A query string is optional. If an extra_query string is not supplied,\n",
    "                        the function will just grab everything from the defined time period. (default: empty string)\n",
    "\n",
    "    Submissions are yielded newest first.\n",
    "\n",
    "    For more information on PRAW, see: https://github.com/praw-dev/praw\n",
    "    For more information on Pushshift, see: https://github.com/pushshift/api\n",
    "    \"\"\"\n",
    "    matching_praw_submissions = []\n",
    "\n",
    "    # Default time values if none are defined (credit to u/bboe's PRAW `submissions()` for this section)\n",
    "    utc_offset = 28800\n",
    "    now = int(time.time())\n",
    "    start = max(int(start) + utc_offset if start else 0, 0)\n",
    "    end = min(int(end) if end else now, now) + utc_offset\n",
    "\n",
    "    # Format our search link properly.\n",
    "    search_link = ('https://api.pushshift.io/reddit/submission/search/'\n",
    "                   '?subreddit={}&after={}&before={}&sort_type=score&sort=asc&limit={}&q={}')\n",
    "    search_link = search_link.format(subreddit, start, end, limit, extra_query)\n",
    "\n",
    "    # Get the data from Pushshift as JSON.\n",
    "    retrieved_data = requests.get(search_link)\n",
    "    returned_submissions = retrieved_data.json()['data']\n",
    "\n",
    "    # Iterate over the returned submissions to convert them to PRAW submission objects.\n",
    "    for submission in returned_submissions:\n",
    "\n",
    "        # Take the ID, fetch the PRAW submission object, and append to our list\n",
    "        praw_submission = reddit.submission(id=submission['id'])\n",
    "        matching_praw_submissions.append(praw_submission)\n",
    "\n",
    "    # Return all PRAW submissions that were obtained.\n",
    "    return matching_praw_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url ='https://api.pushshift.io/reddit/submission/search/?subreddit=Cryptocurrency'\n",
    "requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detail': 'Not authenticated'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url,headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msubmissions_pushshift_praw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCryptoCurrency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [43], line 42\u001b[0m, in \u001b[0;36msubmissions_pushshift_praw\u001b[0;34m(subreddit, start, end, limit, extra_query)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Get the data from Pushshift as JSON.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m retrieved_data \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(search_link)\n\u001b[0;32m---> 42\u001b[0m returned_submissions \u001b[38;5;241m=\u001b[39m \u001b[43mretrieved_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Iterate over the returned submissions to convert them to PRAW submission objects.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m submission \u001b[38;5;129;01min\u001b[39;00m returned_submissions:\n\u001b[1;32m     46\u001b[0m     \n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Take the ID, fetch the PRAW submission object, and append to our list\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "submissions_pushshift_praw('CryptoCurrency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If No SSL Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "url = 'https://www.reddit.com/r/CryptoCurrency/new/'\n",
    "url_test = 'https://www.bbc.co.uk/'\n",
    "\n",
    "# Get url content with beautiful soup\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If SSL Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t3_1ewvczr', 't3_1ewuz5b', 't3_1ewunua']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "url = 'https://www.reddit.com/r/CryptoCurrency/new/'\n",
    "\n",
    "# Create a session with retry mechanism\n",
    "session = requests.Session()\n",
    "retry = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[502, 503, 504]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "try:\n",
    "    response = session.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Your existing code to process soup\n",
    "    data_ks_ids = [a['data-ks-id'] for a in soup.find_all('a', {'data-ks-id': True})]\n",
    "    print(data_ks_ids)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
